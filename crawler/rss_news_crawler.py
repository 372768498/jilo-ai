import feedparser
import time
from datetime import datetime
from supabase import create_client
from config import SUPABASE_URL, SUPABASE_KEY
from processors.translator import translate_text
from openai import OpenAI
import os
import hashlib

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

RSS_SOURCES = {
    'TechCrunch AI': 'https://techcrunch.com/tag/artificial-intelligence/feed/',
    'VentureBeat AI': 'https://venturebeat.com/category/ai/feed/',
    'The Verge AI': 'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
    'MIT Tech Review AI': 'https://www.technologyreview.com/topic/artificial-intelligence/feed/',
    'OpenAI Blog': 'https://openai.com/blog/rss/',
}

def generate_slug(title):
    """ç”Ÿæˆ URL å‹å¥½çš„ slug"""
    slug = title.lower()[:80]
    slug = ''.join(c if c.isalnum() or c == ' ' else '' for c in slug)
    slug = slug.replace(' ', '-').strip('-')
    return slug[:100]

def generate_content_hash(title, source_url):
    """ç”Ÿæˆå†…å®¹å“ˆå¸Œï¼Œç”¨äºå»é‡"""
    content = f"{title}{source_url}"
    return hashlib.md5(content.encode()).hexdigest()

def parse_published_date(entry):
    """è§£æRSSæ¡ç›®çš„å‘å¸ƒæ—¶é—´"""
    for field in ['published_parsed', 'updated_parsed', 'created_parsed']:
        if hasattr(entry, field):
            time_struct = getattr(entry, field)
            if time_struct:
                return datetime(*time_struct[:6]).isoformat()
    return datetime.now().isoformat()

def rewrite_with_ai(title, summary, source_url):
    """ç”¨ AI æ”¹å†™æ–°é—»ï¼Œç”ŸæˆåŸåˆ›å†…å®¹"""
    try:
        prompt = f"""
        Rewrite this AI news in your own words. Make it engaging and SEO-friendly.
        Keep it under 200 words. Focus on the key information.
        
        Original title: {title}
        Original summary: {summary[:500]}
        
        Provide:
        1. A new catchy title (max 100 chars)
        2. A rewritten summary (150-200 words)
        
        Format as:
        TITLE: [new title]
        SUMMARY: [new summary]
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an AI news editor. Rewrite news to be original while keeping facts accurate."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=400
        )
        
        content = response.choices[0].message.content
        
        # è§£æç»“æœ
        lines = content.split('\n')
        new_title = ""
        new_summary = ""
        
        for line in lines:
            if line.startswith('TITLE:'):
                new_title = line.replace('TITLE:', '').strip()
            elif line.startswith('SUMMARY:'):
                new_summary = line.replace('SUMMARY:', '').strip()
        
        return new_title or title, new_summary or summary[:200]
        
    except Exception as e:
        print(f"  âš ï¸  AI rewrite error: {e}")
        return title, summary[:200]

def crawl_rss_news():
    """ä» RSS æŠ“å–å¹¶æ”¹å†™æ–°é—»"""
    print("ğŸ“° Crawling RSS news sources...")
    news_list = []
    
    for source, url in RSS_SOURCES.items():
        try:
            print(f"\nğŸ“¡ Fetching from {source}...")
            feed = feedparser.parse(url)
            
            # æ¯ä¸ªæºå–3æ¡æœ€æ–°çš„
            for entry in feed.entries[:3]:
                try:
                    published_at = parse_published_date(entry)
                    
                    # AI æ”¹å†™
                    print(f"  âœï¸  Rewriting: {entry.title[:50]}...")
                    new_title, new_summary = rewrite_with_ai(
                        entry.title,
                        entry.get('summary', entry.get('description', '')),
                        entry.link
                    )
                    
                    # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                    content_hash = generate_content_hash(new_title, entry.link)
                    
                    news_item = {
                        'title_en': new_title,
                        'summary_en': new_summary,
                        'source': source,
                        'source_url': entry.link,
                        'news_type': 'industry_news',
                        'published_at': published_at,
                        'status': 'published',
                        'content_hash': content_hash  # ç”¨äºå»é‡
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ… Rewritten: {new_title[:50]}")
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"  âŒ Error processing entry: {e}")
                    continue
            
        except Exception as e:
            print(f"âŒ Error fetching {source}: {e}")
            continue
    
    return news_list

def save_news_to_db(news_list):
    """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“ï¼Œé¿å…é‡å¤"""
    if not news_list:
        print("No news to save")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    saved = 0
    skipped = 0
    
    for news in news_list:
        try:
            # ä½¿ç”¨ content_hash å»é‡
            existing = supabase.table('news').select('id').eq('content_hash', news['content_hash']).execute()
            if existing.data:
                print(f"  â­ï¸  Skip (duplicate): {news['title_en'][:50]}")
                skipped += 1
                continue
            
            # ç”Ÿæˆ slug
            slug = generate_slug(news['title_en'])
            
            # ç¿»è¯‘æˆä¸­æ–‡
            print(f"  ğŸŒ Translating: {news['title_en'][:50]}...")
            news['title_zh'] = translate_text(news['title_en'])
            news['summary_zh'] = translate_text(news['summary_en'])
            
            # æ·»åŠ  slug
            news['slug'] = slug
            
            # æ’å…¥æ•°æ®åº“
            result = supabase.table('news').insert(news).execute()
            if result.data:
                saved += 1
                print(f"  âœ… Saved: {news['title_en'][:50]}")
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"  âŒ Error saving: {e}")
            continue
    
    print(f"\nğŸ“Š Summary:")
    print(f"  âœ… Saved: {saved}")
    print(f"  â­ï¸  Skipped (duplicates): {skipped}")
    print(f"  ğŸ“ Total processed: {len(news_list)}")

if __name__ == "__main__":
    print("ğŸš€ Starting RSS news crawler with AI rewriting...")
    print("=" * 60)
    news = crawl_rss_news()
    save_news_to_db(news)
    print("=" * 60)
    print("ğŸ‰ Done!")
