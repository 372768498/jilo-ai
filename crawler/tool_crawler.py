"""
AIå·¥å…·çˆ¬è™« - ä»å¤šä¸ªæ¥æºæŠ“å–å·¥å…·ä¿¡æ¯
"""
import requests
import time
from bs4 import BeautifulSoup
from supabase import create_client
from config import SUPABASE_URL, SUPABASE_KEY
import re
from urllib.parse import urlparse

def generate_slug(name):
    """ç”ŸæˆURLå‹å¥½çš„slug"""
    slug = name.lower()
    slug = re.sub(r'[^\w\s-]', '', slug)
    slug = re.sub(r'[-\s]+', '-', slug)
    return slug[:100]

def get_favicon_url(url):
    """è·å–ç½‘ç«™favicon"""
    try:
        domain = urlparse(url).netloc
        return f"https://www.google.com/s2/favicons?domain={domain}&sz=128"
    except:
        return None

def crawl_producthunt_ai():
    """
    ä»Product HuntæŠ“å–AIå·¥å…·
    æ³¨æ„ï¼šè¿™éœ€è¦Product Hunt API key
    """
    print("\nğŸš€ Crawling Product Hunt AI tools...")
    
    # è¿™é‡Œä½¿ç”¨Product Hunt API
    # éœ€è¦åœ¨ https://www.producthunt.com/v2/oauth/applications ç”³è¯·API key
    
    api_key = os.getenv("PRODUCTHUNT_API_KEY")
    if not api_key:
        print("âš ï¸  No Product Hunt API key found. Skipping...")
        return []
    
    url = "https://api.producthunt.com/v2/api/graphql"
    
    query = """
    query {
      posts(topic: "artificial-intelligence", order: VOTES, first: 20) {
        edges {
          node {
            name
            tagline
            description
            url
            website
            votesCount
            createdAt
            topics {
              edges {
                node {
                  name
                }
              }
            }
          }
        }
      }
    }
    """
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(url, json={"query": query}, headers=headers)
        data = response.json()
        
        tools = []
        for edge in data['data']['posts']['edges']:
            node = edge['node']
            
            tool = {
                'name_en': node['name'],
                'tagline_en': node['tagline'],
                'description_en': node['description'][:500],
                'official_url': node['website'] or node['url'],
                'source': 'Product Hunt',
                'source_url': node['url'],
                'pricing_type': 'freemium',  # é»˜è®¤å€¼
                'category': 'AI Tool',
                'rating': min(5.0, node['votesCount'] / 100),  # ç®€å•è¯„åˆ†
                'status': 'draft'  # éœ€è¦äººå·¥å®¡æ ¸
            }
            
            tools.append(tool)
        
        print(f"âœ… Found {len(tools)} tools from Product Hunt")
        return tools
        
    except Exception as e:
        print(f"âŒ Error crawling Product Hunt: {e}")
        return []

def crawl_theresanaiforthat():
    """
    ä» There's An AI For That æŠ“å–
    æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„çˆ¬è™«ç¤ºä¾‹
    """
    print("\nğŸ¤– Crawling There's An AI For That...")
    
    # è¿™ä¸ªç½‘ç«™æœ‰åçˆ¬è™«ï¼Œéœ€è¦ä½¿ç”¨headerså’Œé€‚å½“çš„å»¶è¿Ÿ
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    tools = []
    
    try:
        # æŠ“å–åˆ†ç±»é¡µé¢
        categories = ['writing', 'image', 'video', 'code', 'chatbot']
        
        for category in categories:
            url = f"https://theresanaiforthat.com/{category}"
            
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´é€‰æ‹©å™¨
            # è¿™åªæ˜¯ç¤ºä¾‹ä»£ç 
            tool_cards = soup.select('.tool-card')[:5]  # æ¯ä¸ªåˆ†ç±»å–5ä¸ª
            
            for card in tool_cards:
                try:
                    name = card.select_one('.tool-name').text.strip()
                    tagline = card.select_one('.tool-tagline').text.strip()
                    url = card.select_one('a')['href']
                    
                    tool = {
                        'name_en': name,
                        'tagline_en': tagline,
                        'description_en': tagline,  # åˆå§‹æè¿°
                        'official_url': url,
                        'source': "There's An AI For That",
                        'source_url': f"https://theresanaiforthat.com{url}",
                        'category': category.title(),
                        'pricing_type': 'freemium',
                        'status': 'draft'
                    }
                    
                    tools.append(tool)
                    
                except Exception as e:
                    continue
            
            time.sleep(2)  # é¿å…è¢«å°IP
        
        print(f"âœ… Found {len(tools)} tools from There's An AI For That")
        return tools
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return []

def crawl_futurepedia():
    """
    ä»FuturepediaæŠ“å–ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    """
    print("\nğŸ”® Crawling Futurepedia...")
    
    # Futurepediaæœ‰APIï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨
    # è¿™é‡Œæ˜¯ç¤ºä¾‹ä»£ç 
    
    url = "https://www.futurepedia.io/api/tools"
    
    try:
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            tools = []
            
            for item in data.get('tools', [])[:20]:
                tool = {
                    'name_en': item.get('name', ''),
                    'tagline_en': item.get('tagline', ''),
                    'description_en': item.get('description', '')[:500],
                    'official_url': item.get('url', ''),
                    'source': 'Futurepedia',
                    'category': item.get('category', 'AI Tool'),
                    'pricing_type': item.get('pricing', 'freemium'),
                    'status': 'draft'
                }
                tools.append(tool)
            
            print(f"âœ… Found {len(tools)} tools from Futurepedia")
            return tools
        
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    return []

def save_tools_to_db(tools):
    """ä¿å­˜å·¥å…·åˆ°æ•°æ®åº“"""
    
    if not tools:
        print("No tools to save")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    saved = 0
    skipped = 0
    
    for tool in tools:
        try:
            # ç”Ÿæˆslug
            slug = generate_slug(tool['name_en'])
            tool['slug'] = slug
            
            # æ·»åŠ logo_url
            if tool.get('official_url'):
                tool['logo_url'] = get_favicon_url(tool['official_url'])
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = supabase.table('tools').select('id').eq('slug', slug).execute()
            
            if existing.data:
                print(f"â­ï¸  Skip (exists): {tool['name_en']}")
                skipped += 1
                continue
            
            # æ’å…¥
            result = supabase.table('tools').insert(tool).execute()
            
            if result.data:
                saved += 1
                print(f"âœ… Saved: {tool['name_en']}")
            
            time.sleep(0.5)
            
        except Exception as e:
            print(f"âŒ Error saving {tool.get('name_en', 'unknown')}: {e}")
    
    print(f"\nğŸ“Š Summary:")
    print(f"   âœ… Saved: {saved}")
    print(f"   â­ï¸  Skipped: {skipped}")
    print(f"   ğŸ“ Total: {len(tools)}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ Starting AI Tool Crawler")
    print("=" * 60)
    
    all_tools = []
    
    # æ–¹æ³•1: Product Hunt (éœ€è¦API key)
    # all_tools.extend(crawl_producthunt_ai())
    
    # æ–¹æ³•2: There's An AI For That (éœ€è¦çˆ¬è™«)
    # all_tools.extend(crawl_theresanaiforthat())
    
    # æ–¹æ³•3: Futurepedia (å¦‚æœæœ‰API)
    # all_tools.extend(crawl_futurepedia())
    
    # ç”±äºè¿™äº›ç½‘ç«™éƒ½æœ‰åçˆ¬è™«æªæ–½ï¼Œæ¨èä½¿ç”¨æ‰‹åŠ¨æ–¹å¼ï¼š
    print("\nğŸ’¡ Recommendation:")
    print("   Instead of scraping, consider:")
    print("   1. Apply for API access (Product Hunt, etc.)")
    print("   2. Manual curation from these sources")
    print("   3. User submissions via your website")
    print("   4. RSS feeds from AI news sites")
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    if all_tools:
        save_tools_to_db(all_tools)
    else:
        print("\nâš ï¸  No tools crawled. See recommendations above.")

if __name__ == "__main__":
    main()