import requests
from bs4 import BeautifulSoup
from supabase import create_client
from config import SUPABASE_URL, SUPABASE_KEY
from processors.translator import translate_text
import time

def crawl_ai_news():
    """çˆ¬å– AI æ–°é—»ï¼ˆç¤ºä¾‹ï¼šä»å¤šä¸ªæ¥æºï¼‰"""
    print("ğŸ“° Crawling AI news...")
    
    news_list = []
    
    # æ¥æº 1: Product Hunt AI è¯é¢˜
    try:
        url = "https://www.producthunt.com/topics/artificial-intelligence"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = soup.find_all('article')[:3]  # å‰3æ¡
        
        for article in articles:
            try:
                title = article.find('h3')
                link = article.find('a')
                
                if title and link:
                    news_item = {
                        'title_en': title.text.strip()[:200],
                        'source': 'Product Hunt',
                        'source_url': f"https://www.producthunt.com{link['href']}" if not link['href'].startswith('http') else link['href'],
                        'news_type': 'product_launch'
                    }
                    news_list.append(news_item)
                    print(f"  âœ“ Found: {news_item['title_en'][:50]}")
            except:
                continue
                
    except Exception as e:
        print(f"  âœ— Product Hunt error: {e}")
    
    return news_list

def save_news_to_db(news_list):
    """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
    if not news_list:
        print("No news to save")
        return
    
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    saved = 0
    
    for news in news_list:
        try:
            # ç”Ÿæˆ slug
            slug = news['title_en'].lower().replace(' ', '-')[:100]
            slug = ''.join(c for c in slug if c.isalnum() or c == '-')
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
            existing = supabase.table('news').select('id').eq('slug', slug).execute()
            if existing.data:
                print(f"  â­ï¸  Skip (exists): {news['title_en'][:50]}")
                continue
            
            # ç¿»è¯‘
            print(f"  ğŸŒ Translating: {news['title_en'][:50]}")
            news['title_zh'] = translate_text(news['title_en'])
            
            # æ·»åŠ å…¶ä»–å­—æ®µ
            news['slug'] = slug
            news['status'] = 'draft'  # å¾…å®¡æ ¸
            
            # æ’å…¥
            result = supabase.table('news').insert(news).execute()
            if result.data:
                saved += 1
                print(f"  âœ… Saved: {news['title_en'][:50]}")
            
            time.sleep(1)
            
        except Exception as e:
            print(f"  âŒ Error saving {news.get('title_en', '')[:30]}: {e}")
    
    print(f"\nâœ… Saved {saved}/{len(news_list)} news items")

if __name__ == "__main__":
    print("ğŸš€ Starting news crawler...")
    news =


 news = crawl_ai_news()
    save_news_to_db(news)
    print("ğŸ‰ Done!")